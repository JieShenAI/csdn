{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:2164] 2025-02-23 23:56:32,992 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1843] 2025-02-23 23:56:33,007 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Downloading Model to directory: /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "2025-02-23 23:56:33,446 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "[INFO|configuration_utils.py:694] 2025-02-23 23:56:33,744 >> loading configuration file /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-23 23:56:33,746 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,750 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,750 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,751 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,751 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,751 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,751 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-23 23:56:33,925 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:694] 2025-02-23 23:56:33,928 >> loading configuration file /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-23 23:56:33,929 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,929 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,929 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,929 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,929 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,929 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:33,929 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-23 23:56:34,085 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-02-23 23:56:34] llamafactory.data.loader:157 >> Loading dataset industry_cls.json...\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 100/100 [00:01<00:00, 87.61 \n",
      "training example:\n",
      "input_ids:\n",
      "[151646, 151644, 112720, 112707, 102679, 44177, 101047, 99717, 70538, 677, 106112, 105223, 101044, 100184, 516, 364, 105787, 100634, 47874, 516, 364, 92894, 38342, 31118, 30858, 118553, 516, 364, 102360, 32648, 100184, 516, 364, 108127, 100404, 516, 364, 104595, 104778, 47874, 516, 364, 102430, 101604, 102891, 101130, 516, 364, 92894, 99404, 11622, 106521, 43316, 107865, 100184, 516, 364, 110135, 103710, 101044, 102966, 516, 364, 99259, 38212, 49082, 51232, 660, 3837, 104931, 90919, 109363, 100409, 16872, 41932, 117735, 102286, 102025, 9370, 99717, 3837, 100155, 80443, 31526, 34794, 44177, 1294, 8997, 102858, 99180, 43316, 56278, 102286, 102025, 5122, 43316, 102858, 104697, 45912, 5373, 43316, 102858, 99361, 5373, 99871, 102858, 100643, 198, 23031, 12669, 44177, 9370, 68805, 66017, 1773, 101912, 5122, 1183, 24048, 497, 330, 24048, 1341, 151645]\n",
      "inputs:\n",
      "<｜begin▁of▁sentence｜><｜User｜>请你逐一浏览列表中的行业分类['印刷专用设备制造', '宠物医院服务', '其他未列明畜牧业', '试验机制造', '失业保险', '临床检验服务', '棉纺纱加工', '其他产业用纺织制成品制造', '家用视听设备批发', '热电联产']，并在其中找出属于下述氢能产业链环节的行业，若没有返回空列表[]。\n",
      "氢气制备产业链环节：制氢原料获取、制氢技术、储氢材料\n",
      "以python列表的格式输出。比如：[\"xxx\", \"xxx\"]<｜Assistant｜>\n",
      "label_ids:\n",
      "[151643]\n",
      "labels:\n",
      "<｜end▁of▁sentence｜>\n",
      "[INFO|configuration_utils.py:694] 2025-02-23 23:56:39,279 >> loading configuration file /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:694] 2025-02-23 23:56:39,279 >> loading configuration file /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-23 23:56:39,280 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|image_processing_auto.py:297] 2025-02-23 23:56:39,280 >> Could not locate the image processor configuration file, will try to use the model config instead.\n",
      "INFO 02-23 23:56:41 config.py:478] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 02-23 23:56:41 arg_utils.py:1086] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-23 23:56:41 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-23 23:56:41 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:41,922 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:41,922 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:41,922 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:41,922 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:41,922 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-02-23 23:56:41,922 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-23 23:56:42,097 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:1093] 2025-02-23 23:56:42,136 >> loading configuration file /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-02-23 23:56:42,136 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151646,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "INFO 02-23 23:56:42 selector.py:120] Using Flash Attention backend.\n",
      "INFO 02-23 23:56:42 model_runner.py:1092] Starting to load model /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.42it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]\n",
      "\n",
      "INFO 02-23 23:56:44 model_runner.py:1097] Loading model weights took 14.2716 GB\n",
      "INFO 02-23 23:56:44 worker.py:241] Memory profiling takes 0.47 seconds\n",
      "INFO 02-23 23:56:44 worker.py:241] the current vLLM instance can use total_gpu_memory (47.41GiB) x gpu_memory_utilization (0.90) = 42.67GiB\n",
      "INFO 02-23 23:56:44 worker.py:241] model weights take 14.27GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 26.87GiB.\n",
      "INFO 02-23 23:56:44 gpu_executor.py:76] # GPU blocks: 31445, # CPU blocks: 4681\n",
      "INFO 02-23 23:56:44 gpu_executor.py:80] Maximum concurrency for 131072 tokens per request: 3.84x\n",
      "INFO 02-23 23:56:46 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-23 23:56:46 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-23 23:56:54 model_runner.py:1527] Graph capturing finished in 9 secs, took 0.23 GiB\n",
      "INFO 02-23 23:56:54 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 10.47 seconds\n",
      "Processed prompts: 100%|█| 100/100 [00:30<00:00,  3.26it/s, est. speed input: 41\n",
      "**********************************************************************\n",
      "100 generated results have been saved at generated_predictions.jsonl.\n",
      "**********************************************************************\n",
      "[rank0]:[W223 23:57:26.860460041 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!python vllm_infer.py \\\n",
    "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "    --template deepseek3 \\\n",
    "    --dataset_dir ../data \\\n",
    "    --dataset industry_cls \\\n",
    "    --max_samples 100 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.7 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --save_name generated_predictions.jsonl\n",
    "    # --fp16 \\\n",
    "    # --gpu_memory_utilization 0.95 \\\n",
    "    # --max_new_tokens 256\n",
    "    \n",
    "    # --do_predict True\n",
    "    # --max_model_len 30624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python vllm_infer.py \\\n",
    "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "    --template deepseek3 \\\n",
    "    --dataset_dir ../data \\\n",
    "    --dataset industry_cls \\\n",
    "    --fp16 \\\n",
    "    --max_samples 100 \\\n",
    "    --flash_attn auto \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.7 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --gpu_memory_utilization 0.95 \\\n",
    "    --max_new_tokens 256\n",
    "    \n",
    "    # --do_predict True\n",
    "    # --max_model_len 30624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/mnt/mydisk/github/csdn/25/02/llm_industry_cls_infer/infer/vllm_infer.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python vllm_infer.py \\\n",
    "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "    --template deepseek3 \\\n",
    "    --dataset_dir ../data \\\n",
    "    --dataset industry_cls \\\n",
    "    --fp16 \\\n",
    "    --max_samples 100 \\\n",
    "    --flash_attn auto \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.7 \\\n",
    "    \n",
    "    # --do_predict True\n",
    "    # --max_model_len 30624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jie/anaconda3/envs/factory/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"_name_or_path\": \"/home/jie/.cache/modelscope/hub/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 5120,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 13824,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"max_window_layers\": 48,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 40,\n",
       "  \"num_hidden_layers\": 48,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.48.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2Config' object has no attribute 'max_model_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_model_len\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/factory/lib/python3.11/site-packages/transformers/configuration_utils.py:211\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    210\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2Config' object has no attribute 'max_model_len'"
     ]
    }
   ],
   "source": [
    "config.max_model_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(vllm_maxlen=4096, vllm_gpu_util=0.9, vllm_enforce_eager=False, vllm_max_lora_rank=32, vllm_config=None, export_dir=None, export_size=5, export_device='cpu', export_quantization_bit=None, export_quantization_dataset=None, export_quantization_nsamples=128, export_quantization_maxlen=1024, export_legacy_format=False, export_hub_model_id=None, image_resolution=589824, video_resolution=65536, video_fps=2.0, video_maxlen=128, quantization_method='bitsandbytes', quantization_bit=None, quantization_type='nf4', double_quantization=True, quantization_device_map=None, model_name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', adapter_name_or_path=None, adapter_folder=None, cache_dir=None, use_fast_tokenizer=True, resize_vocab=False, split_special_tokens=False, new_special_tokens=None, model_revision='main', low_cpu_mem_usage=True, rope_scaling=None, flash_attn='auto', shift_attn=False, mixture_of_depths=None, use_unsloth=False, use_unsloth_gc=False, enable_liger_kernel=False, moe_aux_loss_coef=None, disable_gradient_checkpointing=False, use_reentrant_gc=True, upcast_layernorm=False, upcast_lmhead_output=False, train_from_scratch=False, infer_backend='huggingface', offload_folder='offload', use_cache=True, infer_dtype='auto', hf_hub_token=None, ms_hub_token=None, om_hub_token=None, print_param_status=False, trust_remote_code=False, compute_dtype=None, device_map='auto', model_max_length=None, block_diag_attn=False)\n",
      "GeneratingArguments(do_sample=True, temperature=0.95, top_p=0.7, top_k=50, num_beams=1, max_length=1024, max_new_tokens=1024, repetition_penalty=1.0, length_penalty=1.0, default_system=None, skip_special_tokens=True)\n"
     ]
    }
   ],
   "source": [
    "!python parse_demo.py \\\n",
    "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \\\n",
    "    --template deepseek3 \\\n",
    "    --dataset_dir ../data \\\n",
    "    --dataset industry_cls \\\n",
    "    # --fp16 \\\n",
    "    --max_samples 100 \\\n",
    "    # --flash_attn auto \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.7 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --gpu_memory_utilization 0.95 \\\n",
    "    --max_new_tokens 256 \\\n",
    "    --infer_backend vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelArguments(\n",
    "    vllm_maxlen=4096,\n",
    "    vllm_gpu_util=0.9,\n",
    "    vllm_enforce_eager=False,\n",
    "    vllm_max_lora_rank=32,\n",
    "    vllm_config=None,\n",
    "    export_dir=None,\n",
    "    export_size=5,\n",
    "    export_device=\"cpu\",\n",
    "    export_quantization_bit=None,\n",
    "    export_quantization_dataset=None,\n",
    "    export_quantization_nsamples=128,\n",
    "    export_quantization_maxlen=1024,\n",
    "    export_legacy_format=False,\n",
    "    export_hub_model_id=None,\n",
    "    image_resolution=589824,\n",
    "    video_resolution=65536,\n",
    "    video_fps=2.0,\n",
    "    video_maxlen=128,\n",
    "    quantization_method=\"bitsandbytes\",\n",
    "    quantization_bit=None,\n",
    "    quantization_type=\"nf4\",\n",
    "    double_quantization=True,\n",
    "    quantization_device_map=None,\n",
    "    model_name_or_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    adapter_name_or_path=None,\n",
    "    adapter_folder=None,\n",
    "    cache_dir=None,\n",
    "    use_fast_tokenizer=True,\n",
    "    resize_vocab=False,\n",
    "    split_special_tokens=False,\n",
    "    new_special_tokens=None,\n",
    "    model_revision=\"main\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    rope_scaling=None,\n",
    "    flash_attn=\"auto\",\n",
    "    shift_attn=False,\n",
    "    mixture_of_depths=None,\n",
    "    use_unsloth=False,\n",
    "    use_unsloth_gc=False,\n",
    "    enable_liger_kernel=False,\n",
    "    moe_aux_loss_coef=None,\n",
    "    disable_gradient_checkpointing=False,\n",
    "    use_reentrant_gc=True,\n",
    "    upcast_layernorm=False,\n",
    "    upcast_lmhead_output=False,\n",
    "    train_from_scratch=False,\n",
    "    infer_backend=\"huggingface\",\n",
    "    offload_folder=\"offload\",\n",
    "    use_cache=True,\n",
    "    infer_dtype=\"auto\",\n",
    "    hf_hub_token=None,\n",
    "    ms_hub_token=None,\n",
    "    om_hub_token=None,\n",
    "    print_param_status=False,\n",
    "    trust_remote_code=False,\n",
    "    compute_dtype=None,\n",
    "    device_map=\"auto\",\n",
    "    model_max_length=None,\n",
    "    block_diag_attn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeneratingArguments(\n",
    "    do_sample=True,\n",
    "    temperature=0.95,\n",
    "    top_p=0.7,\n",
    "    top_k=50,\n",
    "    num_beams=1,\n",
    "    max_length=1024,\n",
    "    max_new_tokens=1024,\n",
    "    repetition_penalty=1.0,\n",
    "    length_penalty=1.0,\n",
    "    default_system=None,\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
