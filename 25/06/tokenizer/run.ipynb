{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e787ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/embed/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/embed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae263c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67da96",
   "metadata": {},
   "source": [
    "针对下述data进行tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6c482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"The sun is shining brightly today.\",\n",
    "    \"She enjoys reading books in the park.\",\n",
    "    \"It started raining unexpectedly.\",\n",
    "    \"He ran as fast as he could.\",\n",
    "    \"This restaurant has amazing food.\",\n",
    "    \"We are going on a vacation next week.\",\n",
    "    \"Learning new things is always exciting.\",\n",
    "    \"Please remember to lock the door before leaving.\",\n",
    "    \"They won the first prize in the competition.\",\n",
    "    \"Time flies when you're having fun.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667e9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 11])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 常用的tokenizer写法: 填充到当前batch的最大长度\n",
    "tokenizer(data, padding=True, return_tensors=\"pt\")[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a887b",
   "metadata": {},
   "source": [
    "后续的代码，围绕 `tokenizer.pad` 方法做的展开"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ab70a",
   "metadata": {},
   "source": [
    "tokenizer 不仅可以处理单个字符串，还可以处理字符串列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a656ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 3103, 2003, 9716, 14224, 2651, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符串\n",
    "tokenizer(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807b0322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 3103, 2003, 9716, 14224, 2651, 1012, 102], [101, 2016, 15646, 3752, 2808, 1999, 1996, 2380, 1012, 102], [101, 2009, 2318, 24057, 14153, 1012, 102], [101, 2002, 2743, 2004, 3435, 2004, 2002, 2071, 1012, 102], [101, 2023, 4825, 2038, 6429, 2833, 1012, 102], [101, 2057, 2024, 2183, 2006, 1037, 10885, 2279, 2733, 1012, 102], [101, 4083, 2047, 2477, 2003, 2467, 10990, 1012, 102], [101, 3531, 3342, 2000, 5843, 1996, 2341, 2077, 2975, 1012, 102], [101, 2027, 2180, 1996, 2034, 3396, 1999, 1996, 2971, 1012, 102], [101, 2051, 10029, 2043, 2017, 1005, 2128, 2383, 4569, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符串列表\n",
    "raw_tokens = tokenizer(data)\n",
    "raw_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3565a6",
   "metadata": {},
   "source": [
    "不使用 return_tensor 参数，返回的Dict[str, List]类型，其中的列表不会进行填充。\n",
    "根据下述代码可以发现它们的长度都不一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b8ccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n",
      "7\n",
      "10\n",
      "8\n",
      "11\n",
      "9\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for item in raw_tokens[\"input_ids\"]:\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f30ddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 11])\n"
     ]
    }
   ],
   "source": [
    "# 把它们填充到当前batch的最大长度\n",
    "tokens_pt1 = tokenizer.pad(\n",
    "    raw_tokens,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(tokens_pt1[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bed7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16])\n"
     ]
    }
   ],
   "source": [
    "# 把它们填充到8的倍数\n",
    "tokens_pt_multiple = tokenizer.pad(\n",
    "    raw_tokens,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(tokens_pt_multiple[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96762e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32])\n"
     ]
    }
   ],
   "source": [
    "# 把它们填充到某个固定的最大长度，比如：32。\n",
    "tokens_pt2 = tokenizer.pad(\n",
    "    raw_tokens,\n",
    "    padding=\"max_length\",\n",
    "    max_length=32,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(tokens_pt2[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf7ca23",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "- [https://github.com/FlagOpen/FlagEmbedding/blob/ca91f2b5d10c062c5e3410e28825a1752f0fdada/FlagEmbedding/abc/finetune/embedder/AbsDataset.py](https://github.com/FlagOpen/FlagEmbedding/blob/ca91f2b5d10c062c5e3410e28825a1752f0fdada/FlagEmbedding/abc/finetune/embedder/AbsDataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b87977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
